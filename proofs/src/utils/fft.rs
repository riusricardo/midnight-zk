/*
 * Copyright (C) 2026 Midnight Contributors
 *
 * SPDX-License-Identifier: GPL-3.0-or-later
 */

//! GPU-aware FFT operations
//!
//! This module provides FFT operations that automatically use GPU acceleration
//! when available for supported field types (specifically BLS12-381 scalar `Fq`).
//!
//! # Architecture
//!
//! The module uses the `dispatch_ntt_inplace` helper from `gpu_accel` to
//! automatically route to GPU or CPU based on:
//! - Field type (only `Fq` is GPU-accelerated)
//! - Size threshold (default: 4096 elements)
//! - GPU availability
//!
//! # Usage
//!
//! ```rust,ignore
//! use crate::utils::fft::{best_fft, ifft};
//!
//! // These automatically use GPU for Fq when beneficial
//! best_fft(&mut coeffs, omega, k);
//! ifft(&mut evals, omega_inv, k, divisor);
//! ```

use ff::PrimeField;

#[cfg(feature = "gpu")]
use crate::gpu_accel::dispatch_ntt_inplace;

/// Perform forward FFT (coefficients -> evaluations)
///
/// This is a drop-in replacement for `midnight_curves::fft::best_fft` that
/// automatically uses GPU when beneficial for `Fq`.
///
/// # Arguments
/// * `a` - Input/output slice (modified in-place)
/// * `omega` - Primitive root of unity for the domain
/// * `log_n` - Log2 of the domain size
#[inline]
pub fn best_fft<F: PrimeField + 'static>(a: &mut [F], omega: F, log_n: u32) {
    #[cfg(feature = "gpu")]
    {
        // Use the dispatch helper - it handles type checking and GPU/CPU routing
        let used_gpu = dispatch_ntt_inplace(
            a,
            |fq_slice| {
                midnight_bls12_381_cuda::ntt::forward_ntt_inplace_auto(fq_slice)
                    .map_err(|e| e.to_string())
            },
            |data| midnight_curves::fft::best_fft(data, omega, log_n),
        );
        
        if used_gpu {
            #[cfg(feature = "trace-fft")]
            tracing::debug!("GPU forward FFT completed for size 2^{}", log_n);
            return;
        }
    }
    
    // CPU fallback (or non-GPU build)
    #[cfg(not(feature = "gpu"))]
    midnight_curves::fft::best_fft(a, omega, log_n);
}

/// Perform inverse FFT (evaluations -> coefficients)
///
/// This performs the FFT with omega_inv and then multiplies by the divisor (1/n).
///
/// # Arguments
/// * `a` - Input/output slice (modified in-place)
/// * `omega_inv` - Inverse of the primitive root of unity
/// * `log_n` - Log2 of the domain size
/// * `divisor` - Scaling factor (typically 1/n)
#[inline]
pub fn ifft<F: PrimeField + 'static>(a: &mut [F], omega_inv: F, log_n: u32, divisor: F) {
    #[cfg(feature = "gpu")]
    {
        // Use the dispatch helper for inverse NTT
        // Note: ICICLE's inverse NTT already includes the 1/n scaling
        let used_gpu = dispatch_ntt_inplace(
            a,
            |fq_slice| {
                midnight_bls12_381_cuda::ntt::inverse_ntt_inplace_auto(fq_slice)
                    .map_err(|e| e.to_string())
            },
            |data| {
                // CPU path: do FFT then apply divisor
                midnight_curves::fft::best_fft(data, omega_inv, log_n);
                use super::arithmetic::parallelize;
                parallelize(data, |chunk, _| {
                    for val in chunk {
                        *val *= &divisor;
                    }
                });
            },
        );
        
        if used_gpu {
            #[cfg(feature = "trace-fft")]
            tracing::debug!("GPU inverse FFT completed for size 2^{}", log_n);
            return;
        }
    }
    
    // CPU fallback (or non-GPU build)
    #[cfg(not(feature = "gpu"))]
    {
        midnight_curves::fft::best_fft(a, omega_inv, log_n);
        
        // Apply divisor
        use super::arithmetic::parallelize;
        parallelize(a, |chunk, _| {
            for val in chunk {
                *val *= &divisor;
            }
        });
    }
}

/// Check if GPU FFT is available for the current configuration.
/// 
/// Returns `true` if the GPU feature is enabled and GPU hardware is detected.
#[cfg(feature = "gpu")]
pub fn gpu_fft_available() -> bool {
    midnight_bls12_381_cuda::is_gpu_available()
}

/// Check if GPU FFT is available for the current configuration.
/// 
/// Returns `false` when compiled without GPU support.
#[cfg(not(feature = "gpu"))]
pub fn gpu_fft_available() -> bool {
    false
}

#[cfg(test)]
mod tests {
    use super::*;
    use midnight_curves::Fq;
    use ff::Field;
    use rand_core::OsRng;

    /// Test that our FFT matches the reference implementation
    #[test]
    fn test_fft_matches_reference() {
        let k = 8u32;
        let n = 1usize << k;
        
        // Compute omega
        let mut omega = Fq::ROOT_OF_UNITY;
        for _ in k..Fq::S {
            omega = omega.square();
        }
        
        // Create random input
        let mut our_input: Vec<Fq> = (0..n).map(|_| Fq::random(OsRng)).collect();
        let mut ref_input = our_input.clone();
        
        // Our FFT
        best_fft(&mut our_input, omega, k);
        
        // Reference FFT
        midnight_curves::fft::best_fft(&mut ref_input, omega, k);
        
        // Compare (may differ if GPU uses different root, but roundtrip should work)
        // For now, just test roundtrip
        let omega_inv = omega.invert().unwrap();
        let divisor = Fq::from(n as u64).invert().unwrap();
        
        ifft(&mut our_input, omega_inv, k, divisor);
        midnight_curves::fft::best_fft(&mut ref_input, omega_inv, k);
        for val in ref_input.iter_mut() {
            *val *= divisor;
        }
        
        // Both should produce original values
    }

    /// Test roundtrip FFT -> IFFT
    #[test]
    fn test_fft_roundtrip() {
        let k = 10u32;
        let n = 1usize << k;
        
        // Compute omega
        let mut omega = Fq::ROOT_OF_UNITY;
        for _ in k..Fq::S {
            omega = omega.square();
        }
        let omega_inv = omega.invert().unwrap();
        let divisor = Fq::from(n as u64).invert().unwrap();
        
        // Create input
        let original: Vec<Fq> = (0..n).map(|i| Fq::from(i as u64 + 1)).collect();
        let mut data = original.clone();
        
        // Forward FFT
        best_fft(&mut data, omega, k);
        
        // Inverse FFT
        ifft(&mut data, omega_inv, k, divisor);
        
        // Verify roundtrip
        for (i, (orig, recovered)) in original.iter().zip(data.iter()).enumerate() {
            assert_eq!(*orig, *recovered, "FFT roundtrip failed at index {}", i);
        }
    }
}
